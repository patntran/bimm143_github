---
title: "Class 8: Machine Learning Mini Project"
author: "Patrick Tran"
format: pdf
---

In today's mini-project we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now).

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data.

##Data import

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

Remove the diagnosis column and keep it in a seperate vector for later. 
```{r}
diagnosis <-  as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
head(wisc.df)
```

##Exploratory data analysis

The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data


> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

First find the column names
```{r}
colnames(wisc.data)
```

Next I need to search within the column names for "_mean" pattern. The `grep()` function might help here.

```{r}
inds <- grep("_mean", colnames(wisc.data))
length(inds)
```

> Q4. How many dinmensions are in this dataset?

```{r}
ncol(wisc.data)
```

# Principal Componenet Analysis

First do we need to scale the data before PCA or not.

```{r}
round(apply(wisc.data, 2, sd), 3)
```

Looks like we need to scale. 

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72%

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs capture 91%


## PC plot

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There are too many characters on the plot and they are overlapping each other. This makes it difficult to understand. The x and y axes are difficult to interpret. The plot is too confusing.

We need to make our plot of PC1 vs PC2 (a.k.a score plot, PC-plot, etc.) The main result of PCA...

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis)
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis)
```
The y-values on the y-axis for this plot has increased compared to the previous plot. PC2 shows more variance compared to PC3. 

```{r}
library(ggplot2)

pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <-  diagnosis

ggplot(pc)+
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

## Variance explained

We can get this from the output of the `summary()` function.

```{r}
summary(wisc.pr)
```

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2).

```{r}
#Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing the total variance explained of all principal components

```{r}
pve <- pr.var/ sum(pr.var)
head(pve)
```

```{r}
plot(pve, xlab= "Principal Component", 
     ylab="Proportion of Variance Explained", 
    type="o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

## Examine the PC loadings

How much do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the `$rotation` component of the returned PCA object.

```{r}
head(wisc.pr$rotation[,1:3])
```

Focus in on PC1

```{r}
wisc.pr$rotation[,1]
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean" ,1]
```

There is a complicated mix of variables that go together to make up PC1 - i.e. there are many of the original variables that together contribute highly to PC1.

```{r}
loadings <- as.data.frame(wisc.pr$rotation)
library(ggplot2)
ggplot(loadings) +
  aes(PC1, rownames(loadings)) +
  geom_col()
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance in the data?

5 PCs capture 84.7%.

## Hierarchial Clustering

First scale the wisc.data data and assign the result to data.scaled.

```{r}
data.scaled <- scale(wisc.data)
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist)
```

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At height 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

Cut this tree to yield cluster membership 

```{r}
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
table(cutree(wisc.hclust, k=2), diagnosis)
```

```{r}
table(cutree(wisc.hclust, k=10), diagnosis)
```

The smaller amount of clusters is better to find simpler similarities. The greater the size of the cluster, the more difficult it is to interpret.

> Q13. What is your favorite results for the same data.dist dataset? Explain your reasoning. 

```{r}
hc.complete <- hclust(dist(scale(wisc.data)), method="complete")
plot(hc.complete)
hc.single <- hclust(dist(scale(wisc.data)), method="single")
plot(hc.single)
hc.average <- hclust(dist(scale(wisc.data)), method="average")
plot(hc.average)
hc.ward <- hclust(dist(scale(wisc.data)), method="ward.D2")
plot(hc.ward)
```

I prefer the ward.D2 clustering method because the resulting dendrogram appears the clearest to understand.


# Combine methods: PCA and HCLUST

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps,diagnosis)
```


```{r}
plot(wisc.pr$x[,1])
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`


Try clustering in 3 PCs, that is PC1, PC2 and PC3 as input
```{r}
d <- dist(wisc.pr$x[,1:3])

wisc.pr.hclust <- hclust(d, method="ward.D2")
```

And my tree result figure
```{r}
plot(wisc.pr.hclust)
```

Let's cut this tree into two groups/clusters

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

How well do the two clusters separate the M and B diagnosis?
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q15. How well does the newly created model with four clusters seperate out the two diagnoses?

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters, diagnosis)
```

The newly created model with four clusters separates the two diagnoses. However, with two clusters, the seperation is more clear.

```{r}
(179+333)/nrow(wisc.data)
```


